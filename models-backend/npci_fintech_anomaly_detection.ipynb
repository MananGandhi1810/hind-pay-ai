{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM53WkhKhalZ",
        "outputId": "7d8a361e-5631-4f94-860a-4b821297e8e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-36.1.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n",
            "Downloading Faker-36.1.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-36.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "from scipy.stats import beta, poisson\n",
        "\n",
        "def generate_synthetic_data(df, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    fake = Faker()\n",
        "    Faker.seed(seed)\n",
        "\n",
        "    # Generate realistic user patterns\n",
        "    df['User'] = [fake.uuid4()[:8] for _ in range(len(df))]\n",
        "    df['Card'] = [fake.credit_card_number() for _ in range(len(df))]\n",
        "\n",
        "    # Enhanced temporal patterns\n",
        "    def generate_times(num_users):\n",
        "        # Bimodal distribution for normal vs night users\n",
        "        night_users = np.random.choice([0,1], num_users, p=[0.95, 0.05])\n",
        "        time_params = []\n",
        "\n",
        "        for is_night in night_users:\n",
        "            if is_night:\n",
        "                # Beta distribution for night transactions (10 PM - 4 AM)\n",
        "                alpha, beta_ = 2, 5\n",
        "                base_time = 22 + 6 * beta.rvs(alpha, beta_)\n",
        "            else:\n",
        "                # Bimodal for day transactions (9 AM - 12 PM and 2 PM - 6 PM)\n",
        "                mode = np.random.choice([0,1], p=[0.6, 0.4])\n",
        "                if mode == 0:\n",
        "                    alpha, beta_ = 3, 3  # Morning peak\n",
        "                    base_time = 9 + 3 * beta.rvs(alpha, beta_)\n",
        "                else:\n",
        "                    alpha, beta_ = 2, 4  # Afternoon peak\n",
        "                    base_time = 14 + 4 * beta.rvs(alpha, beta_)\n",
        "\n",
        "            # Add minute variation\n",
        "            minute = np.random.normal(30, 15) % 60\n",
        "            return f\"{int(base_time%24):02d}:{int(minute):02d}\"\n",
        "\n",
        "    # Generate time based on user behavior\n",
        "    user_time_profiles = df.groupby('User')['User'].first().apply(\n",
        "        lambda x: generate_times(1)[0]\n",
        "    )\n",
        "    df = df.merge(user_time_profiles.rename('TimeProfile'), on='User')\n",
        "\n",
        "    # Convert to datetime with realistic patterns\n",
        "    df['Datetime'] = pd.to_datetime(\n",
        "        df[['Year', 'Month', 'Day']].astype(str).agg('-'.join, axis=1) + ' ' + df['TimeProfile']\n",
        "    )\n",
        "\n",
        "    # Generate chargeback patterns\n",
        "    df = df.sort_values(['User', 'Datetime']).reset_index(drop=True)\n",
        "\n",
        "    # Create chargeback flags\n",
        "    df['Chargeback'] = 0\n",
        "    chargeback_users = np.random.choice(\n",
        "        df['User'].unique(),\n",
        "        size=int(len(df['User'].unique())*0.03),  # 3% risky users\n",
        "        replace=False\n",
        "    )\n",
        "\n",
        "    # Track chargeback history per user\n",
        "    user_chargebacks = {user: 0 for user in df['User'].unique()}\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        if row['User'] in chargeback_users:\n",
        "            # Base probability increases with transaction amount\n",
        "            cb_prob = 0.01 + (row['Amount']/df['Amount'].max())*0.15\n",
        "            # Weekend transactions have higher chargeback probability\n",
        "            if row['Datetime'].weekday() >= 5:\n",
        "                cb_prob += 0.05\n",
        "            # Time since last transaction (rush transactions)\n",
        "            if user_chargebacks[row['User']] > 0:\n",
        "                time_since_last = row['Datetime'] - df.iloc[idx-1]['Datetime']\n",
        "                if time_since_last < pd.Timedelta('1h'):\n",
        "                    cb_prob += 0.1\n",
        "\n",
        "            if np.random.rand() < cb_prob:\n",
        "                df.at[idx, 'Chargeback'] = 1\n",
        "                user_chargebacks[row['User']] += 1\n",
        "\n",
        "    # Create rolling chargeback count feature\n",
        "    df['nof.chargebacks'] = df.groupby('User')['Chargeback'].cumsum()\n",
        "\n",
        "    # Add temporal context features\n",
        "    df['Hour'] = df['Datetime'].dt.hour\n",
        "    df['Minute'] = df['Datetime'].dt.minute\n",
        "    df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
        "    df['DaysFromWeekend'] = np.abs(df['DayOfWeek'] - 5)  # Friday/Saturday proximity\n",
        "\n",
        "    # Create cyclical time features\n",
        "    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour']/24)\n",
        "    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour']/24)\n",
        "    df['Minute_sin'] = np.sin(2 * np.pi * df['Minute']/60)\n",
        "    df['Minute_cos'] = np.cos(2 * np.pi * df['Minute']/60)\n",
        "\n",
        "    # Generate time-based anomalies\n",
        "    df['NightTransaction'] = ((df['Hour'] >= 23) | (df['Hour'] <= 4)).astype(int)\n",
        "    df['RushHour'] = ((df['Hour'] >= 7) & (df['Hour'] <= 9)).astype(int)\n",
        "\n",
        "    # Create temporal clusters\n",
        "    conditions = [\n",
        "        (df['Hour'].between(0,4)),   # Late Night\n",
        "        (df['Hour'].between(5,9)),   # Morning\n",
        "        (df['Hour'].between(10,15)), # Midday\n",
        "        (df['Hour'].between(16,19)), # Evening\n",
        "        (df['Hour'].between(20,23)), # Night\n",
        "    ]\n",
        "    df['TimeCluster'] = np.select(conditions, [0,1,2,3,4], default=2)\n",
        "\n",
        "    # Add error patterns correlated with anomalies\n",
        "    df['Errors?'] = np.where(\n",
        "        (df['NightTransaction'] == 1) |\n",
        "        (df['nof.chargebacks'] > 2) |\n",
        "        (df['Amount'] > df['Amount'].quantile(0.98)),\n",
        "        'High Risk Error', 'No Error'\n",
        "    )\n",
        "\n",
        "    return df.drop(columns=['TimeProfile'])"
      ],
      "metadata": {
        "id": "TCN3u8Eghw8N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_with_rl(df, api_key, model_id=\"llama-v3-8b-w\", max_samples=100, batch_size=5):\n",
        "    \"\"\"\n",
        "    Validates generated synthetic data using reinforcement learning via Fireworks AI API.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The synthetic data to validate\n",
        "        api_key (str): Fireworks API key\n",
        "        model_id (str): Fireworks model ID to use\n",
        "        max_samples (int): Maximum number of samples to validate\n",
        "        batch_size (int): Number of samples to process in each batch\n",
        "\n",
        "    Returns:\n",
        "        tuple: (validated_df, validation_scores, feedback)\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Prepare sample data for validation (limit to max_samples)\n",
        "    validation_samples = df.sample(min(len(df), max_samples)).reset_index(drop=True)\n",
        "    validation_scores = []\n",
        "    validation_feedback = []\n",
        "\n",
        "    # Process in batches to avoid overwhelming the API\n",
        "    for i in range(0, len(validation_samples), batch_size):\n",
        "        batch = validation_samples.iloc[i:i+batch_size]\n",
        "\n",
        "        # Convert batch to list of dictionaries for the API\n",
        "        batch_records = batch.to_dict('records')\n",
        "\n",
        "        # Create prompt for the LLM\n",
        "        prompt = f\"\"\"You are a reinforcement learning system validating synthetic financial transaction data.\n",
        "\n",
        "Please analyze these {len(batch_records)} transaction records and evaluate their realism on a scale of 0-10,\n",
        "where 0 means completely unrealistic and 10 means indistinguishable from real data.\n",
        "For each record, provide feedback on:\n",
        "1. Temporal patterns (time of day, day of week)\n",
        "2. User behavior consistency\n",
        "3. Chargeback patterns\n",
        "4. Anomaly indicators\n",
        "\n",
        "For each record, give a score and specific feedback on how to improve realism.\n",
        "\n",
        "Transaction Records:\n",
        "{json.dumps(batch_records, indent=2, default=str)}\n",
        "\n",
        "Respond with a JSON object containing:\n",
        "1. An array of scores (one per record)\n",
        "2. An array of feedback strings (one per record)\n",
        "3. An overall batch score\n",
        "4. Suggestions for improving the data generation model\n",
        "\"\"\"\n",
        "\n",
        "        # Call the Fireworks API\n",
        "        response = requests.post(\n",
        "            f\"https://api.fireworks.ai/inference/v1/completions\",\n",
        "            headers=headers,\n",
        "            json={\n",
        "                \"model\": model_id,\n",
        "                \"prompt\": prompt,\n",
        "                \"max_tokens\": 2048,\n",
        "                \"temperature\": 0.2,\n",
        "                \"top_p\": 0.9\n",
        "            }\n",
        "        )\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"API Error: {response.status_code}\")\n",
        "            print(response.text)\n",
        "            continue\n",
        "\n",
        "        # Parse the response\n",
        "        try:\n",
        "            result = response.json()\n",
        "            llm_response = result.get('choices', [{}])[0].get('text', '{}')\n",
        "\n",
        "            # Extract JSON from the response (handle potential text wrapping)\n",
        "            import re\n",
        "            json_match = re.search(r'({[\\s\\S]*})', llm_response)\n",
        "            if json_match:\n",
        "                validation_result = json.loads(json_match.group(1))\n",
        "\n",
        "                # Store scores and feedback\n",
        "                scores = validation_result.get('scores', [5.0] * len(batch))\n",
        "                feedbacks = validation_result.get('feedback', ['No feedback'] * len(batch))\n",
        "\n",
        "                validation_scores.extend(scores)\n",
        "                validation_feedback.extend(feedbacks)\n",
        "\n",
        "                # Apply reinforcement learning - adjust data based on feedback\n",
        "                for j, (score, feedback) in enumerate(zip(scores, feedbacks)):\n",
        "                    idx = i + j\n",
        "                    if idx < len(validation_samples):\n",
        "                        # Store validation results with the data\n",
        "                        validation_samples.at[idx, 'RL_Score'] = score\n",
        "                        validation_samples.at[idx, 'RL_Feedback'] = feedback\n",
        "\n",
        "                # Allow API to rest between batches\n",
        "                time.sleep(1)\n",
        "            else:\n",
        "                print(\"Failed to extract JSON from response\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing response: {e}\")\n",
        "            print(f\"Response: {llm_response}\")\n",
        "\n",
        "    # Calculate overall validation metrics\n",
        "    if validation_scores:\n",
        "        avg_score = sum(validation_scores) / len(validation_scores)\n",
        "        print(f\"Average validation score: {avg_score:.2f}/10\")\n",
        "\n",
        "        # Apply reinforcement learning adjustments to the entire dataset\n",
        "        df_adjusted = adjust_data_with_rl_feedback(df, validation_samples)\n",
        "\n",
        "        return df_adjusted, validation_scores, validation_feedback\n",
        "\n",
        "    return df, [], []\n",
        "\n",
        "\n",
        "def adjust_data_with_rl_feedback(original_df, validated_samples):\n",
        "    \"\"\"\n",
        "    Applies reinforcement learning feedback to adjust the synthetic data generation.\n",
        "\n",
        "    Args:\n",
        "        original_df (pd.DataFrame): The original synthetic data\n",
        "        validated_samples (pd.DataFrame): Samples with validation scores\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Adjusted synthetic data\n",
        "    \"\"\"\n",
        "    # Create a copy of the original data\n",
        "    adjusted_df = original_df.copy()\n",
        "\n",
        "    # If we don't have enough validation data, return the original\n",
        "    if len(validated_samples) < 10 or 'RL_Score' not in validated_samples.columns:\n",
        "        return adjusted_df\n",
        "\n",
        "    # Identify patterns in low-scoring samples\n",
        "    low_score_threshold = 5.0\n",
        "    low_scoring_samples = validated_samples[validated_samples['RL_Score'] < low_score_threshold]\n",
        "\n",
        "    if len(low_scoring_samples) > 0:\n",
        "        # Analyze temporal patterns\n",
        "        problematic_hours = low_scoring_samples['Hour'].value_counts().index.tolist()[:3]\n",
        "\n",
        "        # Find problematic chargeback patterns\n",
        "        mean_cb_low = low_scoring_samples['Chargeback'].mean()\n",
        "        mean_cb_all = original_df['Chargeback'].mean()\n",
        "\n",
        "        # Adjust data based on findings\n",
        "        if mean_cb_low > mean_cb_all * 1.5:\n",
        "            # Too many chargebacks in the data - reduce them\n",
        "            chargeback_mask = adjusted_df['Chargeback'] == 1\n",
        "            random_indices = np.random.choice(\n",
        "                adjusted_df[chargeback_mask].index,\n",
        "                size=int(sum(chargeback_mask) * 0.2),  # Reduce by 20%\n",
        "                replace=False\n",
        "            )\n",
        "            adjusted_df.loc[random_indices, 'Chargeback'] = 0\n",
        "\n",
        "            # Recalculate the chargeback counts\n",
        "            adjusted_df['nof.chargebacks'] = adjusted_df.groupby('User')['Chargeback'].cumsum()\n",
        "\n",
        "        # Adjust problematic hour distributions\n",
        "        for hour in problematic_hours:\n",
        "            hour_mask = adjusted_df['Hour'] == hour\n",
        "            if sum(hour_mask) > 100:\n",
        "                # Find records in problematic hours and shift them slightly\n",
        "                for idx in adjusted_df[hour_mask].sample(int(sum(hour_mask) * 0.3)).index:\n",
        "                    # Shift hour by ±1-2 hours\n",
        "                    shift = np.random.choice([-2, -1, 1, 2])\n",
        "                    new_hour = (adjusted_df.loc[idx, 'Hour'] + shift) % 24\n",
        "\n",
        "                    # Update hour and datetime\n",
        "                    adjusted_df.at[idx, 'Hour'] = new_hour\n",
        "                    adjusted_df.at[idx, 'Datetime'] = adjusted_df.at[idx, 'Datetime'].replace(\n",
        "                        hour=new_hour\n",
        "                    )\n",
        "\n",
        "                    # Update derived features\n",
        "                    adjusted_df.at[idx, 'Hour_sin'] = np.sin(2 * np.pi * new_hour/24)\n",
        "                    adjusted_df.at[idx, 'Hour_cos'] = np.cos(2 * np.pi * new_hour/24)\n",
        "                    adjusted_df.at[idx, 'NightTransaction'] = ((new_hour >= 23) | (new_hour <= 4)).astype(int)\n",
        "                    adjusted_df.at[idx, 'RushHour'] = ((new_hour >= 7) & (new_hour <= 9)).astype(int)\n",
        "\n",
        "                    # Update time cluster\n",
        "                    if 0 <= new_hour <= 4:\n",
        "                        adjusted_df.at[idx, 'TimeCluster'] = 0\n",
        "                    elif 5 <= new_hour <= 9:\n",
        "                        adjusted_df.at[idx, 'TimeCluster'] = 1\n",
        "                    elif 10 <= new_hour <= 15:\n",
        "                        adjusted_df.at[idx, 'TimeCluster'] = 2\n",
        "                    elif 16 <= new_hour <= 19:\n",
        "                        adjusted_df.at[idx, 'TimeCluster'] = 3\n",
        "                    elif 20 <= new_hour <= 23:\n",
        "                        adjusted_df.at[idx, 'TimeCluster'] = 4\n",
        "\n",
        "        # Re-evaluate error patterns\n",
        "        adjusted_df['Errors?'] = np.where(\n",
        "            (adjusted_df['NightTransaction'] == 1) |\n",
        "            (adjusted_df['nof.chargebacks'] > 2) |\n",
        "            (adjusted_df['Amount'] > adjusted_df['Amount'].quantile(0.98)),\n",
        "            'High Risk Error', 'No Error'\n",
        "        )\n",
        "\n",
        "    return adjusted_df\n",
        "\n",
        "\n",
        "def generate_and_validate_data(base_df, seed=42, api_key=None, iterations=3):\n",
        "    \"\"\"\n",
        "    Generate synthetic data and iteratively improve it using RL validation.\n",
        "\n",
        "    Args:\n",
        "        base_df (pd.DataFrame): Base dataframe with Year, Month, Day, and Amount columns\n",
        "        seed (int): Random seed for reproducibility\n",
        "        api_key (str): Fireworks API key\n",
        "        iterations (int): Number of RL improvement iterations\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The final validated and improved synthetic data\n",
        "    \"\"\"\n",
        "    # Generate initial synthetic data\n",
        "    print(\"Generating initial synthetic data...\")\n",
        "    df = generate_synthetic_data(base_df, seed=seed)\n",
        "\n",
        "    if not api_key:\n",
        "        print(\"No API key provided. Skipping validation.\")\n",
        "        return df\n",
        "\n",
        "    # Iteratively improve data through RL validation\n",
        "    for i in range(iterations):\n",
        "        print(f\"\\nIteration {i+1}/{iterations} of RL validation\")\n",
        "        df, scores, feedback = validate_with_rl(df, api_key, max_samples=min(100, len(df)//10))\n",
        "\n",
        "        if not scores:\n",
        "            print(\"Validation failed. Using current data.\")\n",
        "            break\n",
        "\n",
        "        avg_score = sum(scores) / len(scores)\n",
        "        print(f\"Average validation score: {avg_score:.2f}/10\")\n",
        "\n",
        "        # If we reach a good score, we can stop early\n",
        "        if avg_score >= 8.5:\n",
        "            print(f\"Reached target quality score {avg_score:.2f}. Stopping iterations.\")\n",
        "            break\n",
        "\n",
        "    print(\"Data generation and validation complete.\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "F9xGZTuprKsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data with enhanced features\n",
        "df = pd.read_csv('/content/drive/MyDrive/card_transaction.v1.csv',\n",
        "                         skiprows=lambda x: x > 0 and x % 4 != 0)\n",
        "\n",
        "enhanced_df = generate_and_validate_data(df, api_key=\"fw_3ZZEeumfFNs2Ajn1WJbyQcgC\", iterations=3)\n",
        "\n",
        "# Add anomaly labels combining multiple factors\n",
        "enhanced_df['Is Fraud?'] = np.where(\n",
        "    (enhanced_df['NightTransaction'] &\n",
        "     (enhanced_df['Amount'] > enhanced_df['Amount'].median())) |\n",
        "    (enhanced_df['nof.chargebacks'] > 2) |\n",
        "    (enhanced_df['TimeSinceLastTxn'] < 3600),  # 1 hour\n",
        "    1, 0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "kyc8lNghhxkB",
        "outputId": "3dd9c10c-9a21-46ae-8e64-4847c821d666"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-74935dbe555c>:47: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Datetime'] = pd.to_datetime(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DateParseError",
          "evalue": "Unknown datetime string format, unable to parse: 2002-9-2 1, at position 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDateParseError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-eb1d3bdbdfa8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                          skiprows=lambda x: x > 0 and x % 4 != 0)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0menhanced_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_synthetic_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Add anomaly labels combining multiple factors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-74935dbe555c>\u001b[0m in \u001b[0;36mgenerate_synthetic_data\u001b[0;34m(df, seed)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Convert to datetime with realistic patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     df['Datetime'] = pd.to_datetime(\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TimeProfile'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMutableMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_strptime_with_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     result, tz_parsed = objects_to_datetime64(\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mdayfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[0m\n\u001b[1;32m   2396\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2398\u001b[0;31m     result, tz_parsed = tslib.array_to_datetime(\n\u001b[0m\u001b[1;32m   2399\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mtslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mconversion.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mDateParseError\u001b[0m: Unknown datetime string format, unable to parse: 2002-9-2 1, at position 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enhanced_df.head()"
      ],
      "metadata": {
        "id": "L9O0C7PUpFMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras"
      ],
      "metadata": {
        "id": "WQMJE2NOo9ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = Dense(ff_dim, activation=\"gelu\")(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = Dense(inputs.shape[-1])(x)\n",
        "    return x + res\n",
        "\n",
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    for dim in mlp_units:\n",
        "        x = Dense(dim, activation=\"gelu\")(x)\n",
        "        x = Dropout(mlp_dropout)(x)\n",
        "\n",
        "    outputs = Dense(input_shape[0], activation=\"linear\")(x)  # Reconstruction\n",
        "    return Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "Lej-_z_6nG65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Identify numerical and categorical columns\n",
        "numerical_columns = enhanced_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_columns = enhanced_df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Remove target columns from features\n",
        "columns_to_remove = ['Is Fraud?', 'Chargeback', 'User', 'Card']\n",
        "if 'Datetime' in numerical_columns:\n",
        "    columns_to_remove.append('Datetime')\n",
        "\n",
        "numerical_columns = [col for col in numerical_columns if col not in columns_to_remove]\n",
        "categorical_columns = [col for col in categorical_columns if col not in columns_to_remove]\n",
        "\n",
        "# 2. Create preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_columns),\n",
        "        ('cat', 'drop', categorical_columns)  # Drop categorical for now, could use OneHotEncoder instead\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3. Prepare data\n",
        "X = preprocessor.fit_transform(enhanced_df)\n",
        "input_shape = X.shape[1]\n",
        "\n",
        "# 4. Define model the same way you had it\n",
        "model = build_model(\n",
        "    input_shape=(input_shape,),\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.25,\n",
        "    dropout=0.25,\n",
        ")"
      ],
      "metadata": {
        "id": "0ZKVB4oioeku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=\"mse\",\n",
        "    metrics=[\"mae\"]\n",
        ")"
      ],
      "metadata": {
        "id": "oH3AraFTokOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_data = X[enhanced_df['Is Fraud?'] == 0]\n",
        "history = model.fit(\n",
        "    normal_data, normal_data,\n",
        "    epochs=50,\n",
        "    batch_size=256,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 6. Calculate reconstruction error\n",
        "train_pred = model.predict(normal_data)\n",
        "train_mae = np.mean(np.abs(train_pred - normal_data), axis=1)\n",
        "\n",
        "# 7. Set threshold at 95th percentile\n",
        "threshold = np.percentile(train_mae, 95)\n",
        "\n",
        "# 8. Save threshold for inference\n",
        "import joblib\n",
        "joblib.dump(threshold, 'anomaly_threshold.pkl')\n",
        "\n",
        "# 9. Save entire pipeline\n",
        "import pickle\n",
        "\n",
        "with open('preprocessor.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessor, f)\n",
        "\n",
        "model.save('mtw_fraud_detection_transformer.h5')"
      ],
      "metadata": {
        "id": "s2DJcxbzoHNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data_pred = model.predict(X)\n",
        "all_data_mae = np.mean(np.abs(all_data_pred - X), axis=1)\n",
        "predicted_fraud = all_data_mae > threshold\n",
        "\n",
        "# Calculate accuracy metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Fraud Detection Results:\")\n",
        "print(classification_report(enhanced_df['Is Fraud?'], predicted_fraud))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(enhanced_df['Is Fraud?'], predicted_fraud))"
      ],
      "metadata": {
        "id": "-97j8XPYorRy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}